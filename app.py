import io
import re
import time
import base64
from typing import List, Tuple, Optional, Dict

import streamlit as st
from openai import OpenAI
import fitz  # PyMuPDF
from PIL import Image

from docx import Document
from docx.shared import Pt
from docx.oxml.ns import qn

from streamlit_paste_button import paste_image_button as pbutton


# =========================
# Defaults
# =========================

DEFAULT_BASE_URL = "https://api.sambanova.ai/v1"
DEFAULT_VISION_MODEL = "Llama-4-Maverick-17B-128E-Instruct"
DEFAULT_TEXT_MODEL = "Meta-Llama-3.3-70B-Instruct"

SYSTEM_PROMPT = """B·∫°n l√† tr·ª£ l√Ω chuy·ªÉn ƒë·ªïi t√†i li·ªáu To√°n sang vƒÉn b·∫£n g√µ l·∫°i.
Y√äU C·∫¶U NGHI√äM NG·∫∂T:
1) M·ªçi c√¥ng th·ª©c/to√°n h·ªçc ph·∫£i n·∫±m trong d·∫•u $...$ (inline math).
2) TUY·ªÜT ƒê·ªêI KH√îNG c√≥ k√Ω t·ª± xu·ªëng d√≤ng b√™n trong $...$.
3) Kh√¥ng t·ª± √Ω s·∫Øp x·∫øp l·∫°i th·ª© t·ª±, kh√¥ng ƒë·ªïi s·ªë c√¢u, kh√¥ng g·ªôp/t√°ch c√¢u.
4) Gi·ªØ xu·ªëng d√≤ng l·ªùi gi·∫£i h·ª£p l√≠ (gi·ªëng b·ªë c·ª•c g·ªëc), nh∆∞ng kh√¥ng ƒë∆∞a tab \\t.
5) N·∫øu kh√¥ng ch·∫Øc m·ªôt k√Ω hi·ªáu/to√°n t·ª≠, h√£y gi·ªØ nguy√™n nh∆∞ nh√¨n th·∫•y.
6) ƒê·∫ßu ra ch·ªâ l√† N·ªòI DUNG (plain text), kh√¥ng th√™m ti√™u ƒë·ªÅ/gi·∫£i th√≠ch ngo√†i l·ªÅ.
"""

VISION_USER_INSTRUCTION = """H√£y ƒë·ªçc ch√≠nh x√°c n·ªôi dung trong ·∫£nh v√† g√µ l·∫°i.
- Gi·ªØ nguy√™n th·ª© t·ª± d√≤ng/√Ω/c√¢u.
- V·ªõi m·ªçi bi·ªÉu th·ª©c to√°n h·ªçc: b·ªçc v√†o $...$ v√† ƒë·∫£m b·∫£o kh√¥ng c√≥ xu·ªëng d√≤ng trong $...$.
- Kh√¥ng d√πng \\(\\), \\[\\], $$...$$; ch·ªâ d√πng $...$.
- Kh√¥ng d√πng tab.
Tr·∫£ v·ªÅ ƒë√∫ng n·ªôi dung ƒë√£ g√µ l·∫°i (plain text)."""

TEXT_CLEANUP_INSTRUCTION = """B·∫°n h√£y chu·∫©n h√≥a l·∫°i vƒÉn b·∫£n sau cho ƒë√∫ng y√™u c·∫ßu:
- M·ªçi c√¥ng th·ª©c/to√°n h·ªçc ph·∫£i n·∫±m trong $...$.
- Kh√¥ng c√≥ xu·ªëng d√≤ng trong $...$.
- Kh√¥ng th√™m/b·ªõt √Ω, kh√¥ng ƒë·ªïi th·ª© t·ª±.
- Kh√¥ng d√πng tab.
Ch·ªâ tr·∫£ v·ªÅ vƒÉn b·∫£n ƒë√£ chu·∫©n h√≥a."""


# =========================
# Helpers
# =========================

def make_client(api_key: str, base_url: str) -> OpenAI:
    return OpenAI(api_key=api_key, base_url=base_url)


def encode_image_bytes(img_bytes: bytes, mime: str) -> str:
    b64 = base64.b64encode(img_bytes).decode("utf-8")
    return f"data:{mime};base64,{b64}"


def strip_tabs(text: str) -> str:
    return text.replace("\t", " ").replace("\u000b", " ")


def collapse_newlines_inside_dollars(text: str) -> str:
    """Remove any newline chars inside $...$ blocks."""
    def _fix_block(m: re.Match) -> str:
        inner = m.group(1)
        inner = inner.replace("\r", " ").replace("\n", " ")
        inner = re.sub(r"\s{2,}", " ", inner).strip()
        return f"${inner}$"
    return re.sub(r"\$(.*?)\$", _fix_block, text, flags=re.DOTALL)


def final_sanitize(text: str) -> str:
    text = strip_tabs(text)
    text = collapse_newlines_inside_dollars(text)
    text = re.sub(r"[ \u00A0]{3,}", "  ", text)
    return text.strip()


def pil_to_png_bytes(img: Image.Image) -> bytes:
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()


def pdf_page_count(pdf_bytes: bytes) -> int:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    n = len(doc)
    doc.close()
    return n


def render_pdf_page(pdf_bytes: bytes, page_index: int, dpi: int) -> Image.Image:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    page = doc[page_index]
    zoom = dpi / 72.0
    mat = fitz.Matrix(zoom, zoom)
    pix = page.get_pixmap(matrix=mat, alpha=False)
    img = Image.open(io.BytesIO(pix.tobytes("png"))).convert("RGB")
    doc.close()
    return img


def call_vision_transcribe(
    client: OpenAI,
    model: str,
    image_png_bytes: bytes,
    max_tokens: int,
    temperature: float,
) -> str:
    data_url = encode_image_bytes(image_png_bytes, "image/png")
    resp = client.chat.completions.create(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": VISION_USER_INSTRUCTION},
                    {"type": "image_url", "image_url": {"url": data_url}},
                ],
            },
        ],
    )
    out = resp.choices[0].message.content or ""
    return final_sanitize(out)


def call_text_cleanup(client: OpenAI, model: str, raw_text: str, max_tokens: int) -> str:
    resp = client.chat.completions.create(
        model=model,
        temperature=0.1,
        max_tokens=max_tokens,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": TEXT_CLEANUP_INSTRUCTION + "\n\n---\n\n" + raw_text},
        ],
    )
    out = resp.choices[0].message.content or ""
    return final_sanitize(out)


def transcribe_with_retry(
    client: OpenAI,
    vision_model: str,
    img: Image.Image,
    *,
    max_tokens: int,
    temperature: float,
    retries: int = 2,
    min_chars_ok: int = 40,
) -> Tuple[str, Optional[str]]:
    """
    Returns (text, error_message). error_message None if ok.
    Retry if empty/too short.
    """
    png = pil_to_png_bytes(img)
    last_err = None
    for attempt in range(retries + 1):
        try:
            txt = call_vision_transcribe(client, vision_model, png, max_tokens=max_tokens, temperature=temperature)
            if len(txt.strip()) >= min_chars_ok:
                return txt, None
            last_err = f"K·∫øt qu·∫£ qu√° ng·∫Øn/r·ªóng (len={len(txt.strip())})."
        except Exception as e:
            last_err = f"L·ªói g·ªçi vision: {e}"
        time.sleep(0.6)
    return "", last_err


def build_docx(sections: List[Tuple[str, str]]) -> bytes:
    doc = Document()

    # default font = Times New Roman size 13
    style = doc.styles["Normal"]
    font = style.font
    font.name = "Times New Roman"
    font.size = Pt(13)
    style._element.rPr.rFonts.set(qn("w:eastAsia"), "Times New Roman")

    for idx, (title, content) in enumerate(sections, start=1):
        if title:
            p = doc.add_paragraph()
            r = p.add_run(title)
            r.bold = True
            r.font.name = "Times New Roman"
            r.font.size = Pt(13)
            r._element.rPr.rFonts.set(qn("w:eastAsia"), "Times New Roman")

        lines = content.splitlines() if content else []
        if not lines:
            doc.add_paragraph("")
        else:
            for line in lines:
                doc.add_paragraph(line)

        if idx != len(sections):
            doc.add_page_break()

    buf = io.BytesIO()
    doc.save(buf)
    return buf.getvalue()


# =========================
# Streamlit UI
# =========================

st.set_page_config(page_title="·∫¢nh/PDF ‚Üí Word (SambaNova)", layout="wide")
st.title("üìÑ ·∫¢nh / PDF ‚Üí Word (.docx) b·∫±ng SambaNova")
st.caption("Nghi√™m ng·∫∑t: c√¥ng th·ª©c to√°n trong $...$ v√† kh√¥ng xu·ªëng d√≤ng b√™n trong $...$.")

with st.sidebar:
    st.header("C·∫•u h√¨nh API")
    api_key = st.text_input("SambaNova API Key", type="password", placeholder="Nh·∫≠p key c·ªßa b·∫°n‚Ä¶")
    base_url = st.text_input("Base URL", value=DEFAULT_BASE_URL)
    vision_model = st.text_input("Vision model", value=DEFAULT_VISION_MODEL)
    text_model = st.text_input("Text model (cleanup)", value=DEFAULT_TEXT_MODEL)

    st.divider()
    st.subheader("Ch·∫•t l∆∞·ª£ng ƒë·ªçc PDF")
    dpi_main = st.slider("DPI ch√≠nh", 120, 320, 240, 10)
    dpi_fallback = st.slider("DPI fallback (n·∫øu trang l·ªói/r·ªóng)", 120, 320, 180, 10)

    st.divider()
    st.subheader("Gi·ªõi h·∫°n tr·∫£ l·ªùi")
    vision_max_tokens = st.slider("Vision max_tokens / trang", 1500, 9000, 6500, 250)
    cleanup_max_tokens = st.slider("Cleanup max_tokens / trang", 1500, 9000, 4500, 250)
    temperature = st.slider("temperature", 0.0, 0.8, 0.2, 0.05)

    st.divider()
    do_cleanup = st.toggle("Chu·∫©n ho√° l·∫°i (khuy·∫øn ngh·ªã)", value=True)
    min_chars_ok = st.slider("Ng∆∞·ª°ng t·ªëi thi·ªÉu k√Ω t·ª± ƒë·ªÉ coi l√† OK", 10, 200, 40, 5)
    retries = st.slider("S·ªë l·∫ßn retry n·∫øu trang r·ªóng", 0, 4, 2, 1)

tabs = st.tabs(["üìé T·∫£i file (PDF/·∫¢nh)", "üìã D√°n ·∫£nh (Ctrl+V)"])

uploads = []
pasted_images: List[Tuple[str, bytes]] = []

with tabs[0]:
    st.subheader("T·∫£i t·ªáp")
    uploads = st.file_uploader(
        "Ch·ªçn 1 ho·∫∑c nhi·ªÅu t·ªáp (PDF/PNG/JPG/JPEG)",
        type=["pdf", "png", "jpg", "jpeg"],
        accept_multiple_files=True,
    )

with tabs[1]:
    st.subheader("D√°n ·∫£nh t·ª´ clipboard")
    st.caption("B·∫•m n√∫t d∆∞·ªõi ƒë√¢y r·ªìi Ctrl+V ƒë·ªÉ d√°n ·∫£nh (tr√¨nh duy·ªát c·∫ßn cho ph√©p Clipboard).")

    paste_result = pbutton(
        label="üìã D√°n ·∫£nh (Ctrl+V)",
        key="paste_btn_1",
        errors="ignore",
    )

    if paste_result.image_data is not None:
        img = paste_result.image_data.convert("RGB")
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        img_bytes = buf.getvalue()
        pasted_images.append(("pasted_image_1.png", img_bytes))
        st.image(img_bytes, caption="·∫¢nh v·ª´a d√°n", use_column_width=True)

have_inputs = (uploads and len(uploads) > 0) or (len(pasted_images) > 0)

if st.button("üöÄ Chuy·ªÉn sang Word", type="primary", disabled=(not have_inputs or not api_key)):
    client = make_client(api_key, base_url)

    sections: List[Tuple[str, str]] = []
    report_rows: List[Dict[str, str]] = []

    # ƒë·∫øm t·ªïng job (∆∞·ªõc l∆∞·ª£ng ƒë·ªÉ progress)
    total_jobs = 0
    if uploads:
        for up in uploads:
            if up.name.lower().endswith(".pdf"):
                b = up.read()
                total_jobs += max(1, pdf_page_count(b))
                up.seek(0)
            else:
                total_jobs += 1
    total_jobs += len(pasted_images)

    progress = st.progress(0)
    done = 0

    # -------- Handle uploads --------
    if uploads:
        for up in uploads:
            filename = up.name
            data = up.read()

            if filename.lower().endswith(".pdf"):
                st.write(f"### üìé PDF: {filename}")
                n_pages = pdf_page_count(data)
                st.write(f"- S·ªë trang PDF: **{n_pages}**")

                page_texts: List[str] = []
                for pi in range(n_pages):
                    page_no = pi + 1

                    # Render page (try main dpi; fallback if render error)
                    try:
                        img = render_pdf_page(data, pi, dpi=dpi_main)
                        used_dpi = dpi_main
                    except Exception as e:
                        try:
                            img = render_pdf_page(data, pi, dpi=dpi_fallback)
                            used_dpi = dpi_fallback
                        except Exception as e2:
                            report_rows.append({
                                "File": filename,
                                "Trang": str(page_no),
                                "Tr·∫°ng th√°i": "‚ùå Render l·ªói",
                                "Ghi ch√∫": f"{e} | fallback: {e2}"
                            })
                            page_texts.append("")  # gi·ªØ ch·ªó => kh√¥ng r·ª•ng trang
                            done += 1
                            progress.progress(min(1.0, done / max(1, total_jobs)))
                            continue

                    with st.spinner(f"ƒêang ƒë·ªçc {filename} ‚Äî trang {page_no}/{n_pages} (DPI {used_dpi}) ‚Ä¶"):
                        txt, err = transcribe_with_retry(
                            client,
                            vision_model,
                            img,
                            max_tokens=vision_max_tokens,
                            temperature=temperature,
                            retries=retries,
                            min_chars_ok=min_chars_ok,
                        )

                        # N·∫øu r·ªóng => th·ª≠ fallback DPI (n·∫øu ch∆∞a d√πng)
                        if (not txt.strip()) and (dpi_fallback != used_dpi):
                            with st.spinner(f"Trang {page_no} r·ªóng ‚Üí th·ª≠ l·∫°i DPI {dpi_fallback} ‚Ä¶"):
                                try:
                                    img2 = render_pdf_page(data, pi, dpi=dpi_fallback)
                                    txt2, err2 = transcribe_with_retry(
                                        client, vision_model, img2,
                                        max_tokens=vision_max_tokens,
                                        temperature=temperature,
                                        retries=retries,
                                        min_chars_ok=min_chars_ok,
                                    )
                                    if txt2.strip():
                                        txt, err = txt2, None
                                    else:
                                        err = err2 or err
                                except Exception as e3:
                                    err = f"{err} | fallback render error: {e3}"

                        # Cleanup theo t·ª´ng trang ƒë·ªÉ tr√°nh ‚Äúr·ª•ng‚Äù
                        if do_cleanup and txt.strip():
                            with st.spinner(f"Chu·∫©n ho√° trang {page_no} ‚Ä¶"):
                                try:
                                    txt = call_text_cleanup(client, text_model, txt, max_tokens=cleanup_max_tokens)
                                except Exception as e:
                                    report_rows.append({
                                        "File": filename,
                                        "Trang": str(page_no),
                                        "Tr·∫°ng th√°i": "‚ö†Ô∏è Cleanup l·ªói",
                                        "Ghi ch√∫": str(e)
                                    })

                        status = "‚úÖ OK" if txt.strip() else "‚ö†Ô∏è R·ªóng"
                        note = "" if txt.strip() else (err or "Kh√¥ng r√µ l√Ω do")

                        report_rows.append({
                            "File": filename,
                            "Trang": str(page_no),
                            "Tr·∫°ng th√°i": status,
                            "Ghi ch√∫": note
                        })

                        # lu√¥n append ƒë·ªÉ kh√¥ng m·∫•t trang
                        page_texts.append(txt.strip())

                    done += 1
                    progress.progress(min(1.0, done / max(1, total_jobs)))

                # gh√©p theo trang (lu√¥n ƒë·ªß Trang 1..n)
                merged_pages = []
                for i, t in enumerate(page_texts, start=1):
                    merged_pages.append(f"[Trang {i}]\n{t}".strip())
                merged = "\n\n".join(merged_pages).strip()
                sections.append((filename, merged))

            else:
                st.write(f"### üñºÔ∏è ·∫¢nh: {filename}")
                try:
                    img = Image.open(io.BytesIO(data)).convert("RGB")
                except Exception as e:
                    report_rows.append({"File": filename, "Trang": "-", "Tr·∫°ng th√°i": "‚ùå ·∫¢nh l·ªói", "Ghi ch√∫": str(e)})
                    done += 1
                    progress.progress(min(1.0, done / max(1, total_jobs)))
                    continue

                with st.spinner("ƒêang ƒë·ªçc ·∫£nh‚Ä¶"):
                    txt, err = transcribe_with_retry(
                        client, vision_model, img,
                        max_tokens=vision_max_tokens,
                        temperature=temperature,
                        retries=retries,
                        min_chars_ok=min_chars_ok,
                    )
                    if do_cleanup and txt.strip():
                        try:
                            txt = call_text_cleanup(client, text_model, txt, max_tokens=cleanup_max_tokens)
                        except Exception as e:
                            report_rows.append({"File": filename, "Trang": "-", "Tr·∫°ng th√°i": "‚ö†Ô∏è Cleanup l·ªói", "Ghi ch√∫": str(e)})

                report_rows.append({
                    "File": filename,
                    "Trang": "-",
                    "Tr·∫°ng th√°i": "‚úÖ OK" if txt.strip() else "‚ö†Ô∏è R·ªóng",
                    "Ghi ch√∫": "" if txt.strip() else (err or "Kh√¥ng r√µ l√Ω do")
                })
                sections.append((filename, txt))

                done += 1
                progress.progress(min(1.0, done / max(1, total_jobs)))

    # -------- Handle pasted images --------
    for name, b in pasted_images:
        st.write(f"### üìã ·∫¢nh d√°n: {name}")
        try:
            img = Image.open(io.BytesIO(b)).convert("RGB")
        except Exception as e:
            report_rows.append({"File": name, "Trang": "-", "Tr·∫°ng th√°i": "‚ùå ·∫¢nh d√°n l·ªói", "Ghi ch√∫": str(e)})
            done += 1
            progress.progress(min(1.0, done / max(1, total_jobs)))
            continue

        with st.spinner("ƒêang ƒë·ªçc ·∫£nh d√°n‚Ä¶"):
            txt, err = transcribe_with_retry(
                client, vision_model, img,
                max_tokens=vision_max_tokens,
                temperature=temperature,
                retries=retries,
                min_chars_ok=min_chars_ok,
            )
            if do_cleanup and txt.strip():
                try:
                    txt = call_text_cleanup(client, text_model, txt, max_tokens=cleanup_max_tokens)
                except Exception as e:
                    report_rows.append({"File": name, "Trang": "-", "Tr·∫°ng th√°i": "‚ö†Ô∏è Cleanup l·ªói", "Ghi ch√∫": str(e)})

        report_rows.append({
            "File": name,
            "Trang": "-",
            "Tr·∫°ng th√°i": "‚úÖ OK" if txt.strip() else "‚ö†Ô∏è R·ªóng",
            "Ghi ch√∫": "" if txt.strip() else (err or "Kh√¥ng r√µ l√Ω do")
        })
        sections.append((name, txt))

        done += 1
        progress.progress(min(1.0, done / max(1, total_jobs)))

    # Build Word
    with st.spinner("ƒêang t·∫°o Word‚Ä¶"):
        docx_bytes = build_docx(sections)

    st.success("Xong! T·∫£i Word b√™n d∆∞·ªõi. (C√≥ b√°o c√°o trang n√†o r·ªóng/l·ªói ƒë·ªÉ ki·ªÉm tra.)")
    st.download_button(
        "‚¨áÔ∏è T·∫£i Word (.docx)",
        data=docx_bytes,
        file_name="output.docx",
        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    )

    st.subheader("üìã B√°o c√°o ƒë·ªçc trang")
    if report_rows:
        st.dataframe(report_rows, use_container_width=True)

else:
    if not api_key:
        st.info("Nh·∫≠p SambaNova API Key ·ªü sidebar.")
    elif not have_inputs:
        st.info("T·∫£i PDF/·∫£nh ho·∫∑c d√°n ·∫£nh (Ctrl+V) ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
