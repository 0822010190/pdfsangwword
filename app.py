import io
import os
import json
from typing import List, Tuple, Optional

import streamlit as st
import requests
from PIL import Image
import fitz  # PyMuPDF
import pytesseract

from docx import Document
from docx.shared import Inches
from docx.oxml.ns import qn
from docx.shared import Pt

from streamlit_paste_button import paste_image_button as paste_btn


# =========================
# SambaNova (OpenAI-compatible)
# =========================
SAMBANOVA_BASE_URL = "https://api.sambanova.ai/v1"
DEFAULT_MODEL = "Meta-Llama-3.3-70B-Instruct"


def sambanova_chat(api_key: str, model: str, messages: List[dict], timeout: int = 90) -> str:
    if not api_key:
        raise RuntimeError("Ch∆∞a nh·∫≠p SambaNova API Key.")

    url = f"{SAMBANOVA_BASE_URL}/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model, "messages": messages, "temperature": 0.1, "max_tokens": 2048}

    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=timeout)
    if r.status_code != 200:
        raise RuntimeError(f"SambaNova API l·ªói {r.status_code}: {r.text}")

    data = r.json()
    return data["choices"][0]["message"]["content"]


SYSTEM_RULES = """
B·∫°n l√† c√¥ng c·ª• chu·∫©n ho√° vƒÉn b·∫£n t·ª´ PDF/OCR ƒë·ªÉ ƒë∆∞a v√†o Word.

Y√äU C·∫¶U NGHI√äM NG·∫∂T:
1) GI·ªÆ NGUY√äN N·ªòI DUNG: kh√¥ng th√™m/b·ªõt/suy di·ªÖn.
2) GI·ªÆ NGUY√äN XU·ªêNG D√íNG: kh√¥ng t·ª± g·ªôp d√≤ng, kh√¥ng t·ª± ng·∫Øt d√≤ng l·∫°i.
3) M·ªåI BI·ªÇU TH·ª®C/CT TO√ÅN PH·∫¢I N·∫∞M TRONG $...$.
   - N·∫øu ƒë√£ l√† LaTeX th√¨ v·∫´n ph·∫£i b·ªçc $...$ (n·∫øu ch∆∞a b·ªçc).
   - Kh√¥ng d√πng \\( \\) ho·∫∑c \\[ \\] ho·∫∑c $$ $$.
4) VƒÉn b·∫£n th∆∞·ªùng KH√îNG ƒë·∫∑t trong $...$.
5) Ch·ªâ tr·∫£ v·ªÅ TEXT THU·∫¶N, kh√¥ng markdown, kh√¥ng th√™m ti√™u ƒë·ªÅ.
""".strip()


def normalize_with_ai(api_key: str, model: str, raw_text: str) -> str:
    messages = [
        {"role": "system", "content": SYSTEM_RULES},
        {"role": "user", "content": raw_text},
    ]
    return sambanova_chat(api_key, model, messages).strip()


# =========================
# OCR (FAIL-SOFT)
# =========================
def ocr_image_pil(img: Image.Image) -> str:
    """OCR b·∫±ng Tesseract. N·∫øu thi·∫øu Tesseract (Streamlit Cloud), tr·∫£ v·ªÅ '' v√† KH√îNG l√†m app ch·∫øt."""
    try:
        return pytesseract.image_to_string(img) or ""
    except Exception as e:
        st.warning(
            "‚ö†Ô∏è OCR kh√¥ng ch·∫°y ƒë∆∞·ª£c (thi·∫øu Tesseract tr√™n m√¥i tr∆∞·ªùng deploy). "
            "App s·∫Ω b·ªè qua OCR ƒë·ªÉ kh√¥ng b·ªã l·ªói.\n"
            f"Chi ti·∫øt: {e}"
        )
        return ""


# =========================
# PDF extraction: "gi·ªëng b·∫£n ƒë·∫ßu"
# - ∆Øu ti√™n text layer
# - Trang kh√¥ng c√≥ text: render ·∫£nh; OCR ch·ªâ khi b·∫≠t
# =========================
def render_page_image(page: fitz.Page, dpi: int = 200) -> Image.Image:
    zoom = dpi / 72
    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), alpha=False)
    return Image.open(io.BytesIO(pix.tobytes("png"))).convert("RGB")


def extract_pdf_pages(
    pdf_bytes: bytes,
    dpi: int = 200,
    use_ocr: bool = False,
    text_min_chars: int = 30,
) -> List[dict]:
    """
    Tr·∫£ v·ªÅ list page items:
      {
        "page_index": int,
        "text": str,               # text layer ho·∫∑c OCR (n·∫øu b·∫≠t)
        "has_text": bool,          # c√≥ text layer ƒë·ªß ng∆∞·ª°ng
        "image": PIL.Image         # ·∫£nh trang ƒë·ªÉ ƒë√≠nh k√®m
      }
    """
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    out = []

    for i in range(len(doc)):
        page = doc.load_page(i)

        text_layer = (page.get_text("text") or "").replace("\r\n", "\n").replace("\r", "\n").strip()
        has_text = len(text_layer) >= text_min_chars

        img = render_page_image(page, dpi=dpi)

        if has_text:
            text = text_layer
        else:
            text = ocr_image_pil(img).strip() if use_ocr else ""

        out.append({"page_index": i, "text": text, "has_text": has_text, "image": img})

    doc.close()
    return out


# =========================
# Images & clipboard
# =========================
def bytes_to_pil(b: bytes) -> Image.Image:
    return Image.open(io.BytesIO(b)).convert("RGB")


# =========================
# Word export
# =========================
def set_doc_default_font(doc: Document, font_name: str = "Times New Roman", font_size_pt: int = 13):
    style = doc.styles["Normal"]
    font = style.font
    font.name = font_name
    font.size = Pt(font_size_pt)
    # Ensure East Asia font as well
    style._element.rPr.rFonts.set(qn("w:eastAsia"), font_name)


def add_text_preserve_lines(doc: Document, text: str):
    text = (text or "").replace("\r\n", "\n").replace("\r", "\n")
    for line in text.split("\n"):
        doc.add_paragraph(line)


def build_docx_from_pdf_pages(
    title: str,
    pages: List[dict],
    normalized_texts: Optional[List[str]],
    embed_images: bool,
    page_image_width_in: float = 6.5,
) -> bytes:
    """
    N·∫øu normalized_texts != None: d√πng text ƒë√£ chu·∫©n ho√° theo AI theo t·ª´ng trang (c√πng s·ªë l∆∞·ª£ng pages).
    """
    doc = Document()
    set_doc_default_font(doc, "Times New Roman", 13)

    if title.strip():
        doc.add_heading(title.strip(), level=1)

    for idx, p in enumerate(pages):
        page_no = p["page_index"] + 1

        doc.add_paragraph(f"--- Trang {page_no} ---")

        # ƒê√≠nh k√®m ·∫£nh trang (kh√¥ng crop; add_picture ch·ªâ resize theo width)
        if embed_images and p.get("image") is not None:
            buf = io.BytesIO()
            p["image"].save(buf, format="PNG")
            buf.seek(0)
            doc.add_picture(buf, width=Inches(page_image_width_in))

        # Text cho trang
        text_to_write = ""
        if normalized_texts is not None:
            text_to_write = normalized_texts[idx] or ""
        else:
            text_to_write = p.get("text", "") or ""

        if text_to_write.strip():
            add_text_preserve_lines(doc, text_to_write)
        else:
            # Kh√¥ng c√≥ text => ghi ch√∫ r√µ r√†ng ƒë·ªÉ th·∫ßy th·∫•y "kh√¥ng b·ªã c·∫Øt", ch·ªâ l√† trang scan ch∆∞a OCR
            if p.get("has_text", False):
                doc.add_paragraph("(Trang n√†y c√≥ text layer nh∆∞ng kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c n·ªôi dung.)")
            else:
                doc.add_paragraph("(Trang d·∫°ng ·∫£nh/scan: ch∆∞a c√≥ text. B·∫≠t OCR n·∫øu mu·ªën th·ª≠ ƒë·ªçc ch·ªØ.)")

    out = io.BytesIO()
    doc.save(out)
    return out.getvalue()


def build_docx_from_images(
    title: str,
    images: List[Image.Image],
    use_ocr: bool,
    use_ai: bool,
    api_key: str,
    model: str,
    embed_images: bool,
    image_width_in: float = 6.5,
) -> Tuple[bytes, str]:
    """
    D√πng cho ·∫£nh upload/paste: OCR (n·∫øu b·∫≠t) -> AI (n·∫øu b·∫≠t).
    Tr·∫£ v·ªÅ (docx_bytes, preview_text).
    """
    raw_texts = []
    for img in images:
        raw_texts.append(ocr_image_pil(img).strip() if use_ocr else "")

    raw_text = "\n\n".join([t for t in raw_texts if t]).strip()

    final_text = raw_text
    if use_ai and raw_text.strip():
        final_text = normalize_with_ai(api_key, model, raw_text)

    doc = Document()
    set_doc_default_font(doc, "Times New Roman", 13)

    if title.strip():
        doc.add_heading(title.strip(), level=1)

    if embed_images:
        doc.add_paragraph("·∫¢nh ƒë√≠nh k√®m:")
        for im in images:
            buf = io.BytesIO()
            im.save(buf, format="PNG")
            buf.seek(0)
            doc.add_picture(buf, width=Inches(image_width_in))

    doc.add_paragraph("N·ªôi dung tr√≠ch xu·∫•t:")
    if final_text.strip():
        add_text_preserve_lines(doc, final_text)
    else:
        doc.add_paragraph("(Ch∆∞a c√≥ text. B·∫≠t OCR ƒë·ªÉ th·ª≠ tr√≠ch xu·∫•t ch·ªØ t·ª´ ·∫£nh.)")

    out = io.BytesIO()
    doc.save(out)
    return out.getvalue(), final_text


# =========================
# UI
# =========================
st.set_page_config(page_title="PDF/·∫¢nh ‚Üí Word (b·∫£n gi·ªëng b·∫£n ƒë·∫ßu + an to√†n)", layout="wide")
st.title("üìÑ PDF/·∫¢nh ‚Üí Word (.docx) ‚Äî gi·ªëng b·∫£n ƒë·∫ßu nh∆∞ng an to√†n")

with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    api_key = st.text_input("SambaNova API Key", type="password", value=os.getenv("SAMBANOVA_API_KEY", ""))
    model = st.text_input("Model", value=DEFAULT_MODEL)

    use_ai = st.checkbox("D√πng AI ƒë·ªÉ √©p c√¥ng th·ª©c v√†o $...$ (khuy·∫øn ngh·ªã)", value=True)
    use_ocr = st.checkbox("D√πng OCR (c·∫ßn Tesseract, Streamlit Cloud th∆∞·ªùng KH√îNG c√≥)", value=False)

    embed_images = st.checkbox("ƒê√≠nh k√®m ·∫£nh trang v√†o Word", value=True)
    dpi = st.slider("DPI render PDF (ƒë·ªÉ ·∫£nh r√µ h∆°n)", 120, 300, 200, 10)

    st.caption(
        "Logic gi·ªëng b·∫£n ƒë·∫ßu: ∆∞u ti√™n text layer. Trang scan s·∫Ω kh√¥ng OCR n·∫øu t·∫Øt OCR, "
        "nh∆∞ng v·∫´n ƒë√≠nh k√®m ·∫£nh + ghi ch√∫ ƒë·ªÉ kh√¥ng m·∫•t trang."
    )

col1, col2 = st.columns([1, 1], gap="large")

with col1:
    st.subheader("1) Nh·∫≠p d·ªØ li·ªáu")
    st.markdown("**A) D√°n ·∫£nh (Ctrl+V)**: b·∫•m n√∫t r·ªìi Ctrl+V.")
    paste = paste_btn("üìã Paste ·∫£nh (Ctrl+V)")
    pasted_images: List[Image.Image] = []
    if paste.image_data is not None:
        pasted_images.append(paste.image_data)
        st.image(paste.image_data, caption="·∫¢nh d√°n", use_container_width=True)

    st.markdown("**B) Upload PDF / ·∫£nh**")
    uploads = st.file_uploader("Ch·ªçn PDF ho·∫∑c ·∫£nh", type=["pdf", "png", "jpg", "jpeg"], accept_multiple_files=True)

with col2:
    st.subheader("2) Xu·∫•t Word")
    title = st.text_input("Ti√™u ƒë·ªÅ trong Word (tu·ª≥ ch·ªçn)", value="")

    run = st.button("üöÄ Chuy·ªÉn ƒë·ªïi", type="primary")

    if run:
        if (not uploads) and (not pasted_images):
            st.error("Ch∆∞a c√≥ file/·∫£nh.")
            st.stop()

        pdf_pages_all: List[dict] = []
        img_only: List[Image.Image] = []

        # Collect uploads
        if uploads:
            for f in uploads:
                data = f.read()
                if f.name.lower().endswith(".pdf"):
                    with st.spinner(f"ƒêang x·ª≠ l√Ω PDF: {f.name} ..."):
                        pages = extract_pdf_pages(
                            data,
                            dpi=dpi,
                            use_ocr=use_ocr,      # an to√†n: fail-soft n·∫øu thi·∫øu tesseract
                            text_min_chars=30     # "gi·ªëng b·∫£n ƒë·∫ßu": ch·ªâ coi l√† c√≥ text khi ƒë·ªß ng∆∞·ª°ng
                        )
                        pdf_pages_all.extend(pages)
                else:
                    img_only.append(bytes_to_pil(data))

        # Collect pasted images
        img_only.extend(pasted_images)

        # ===== PDF ‚Üí Word =====
        if pdf_pages_all:
            # Chu·∫©n ho√° AI theo t·ª´ng trang (ch·ªâ nh·ªØng trang c√≥ text)
            normalized_per_page: Optional[List[str]] = None
            if use_ai and api_key.strip():
                with st.spinner("ƒêang chu·∫©n ho√° $...$ b·∫±ng SambaNova (ch·ªâ tr√™n ph·∫ßn text tr√≠ch xu·∫•t) ..."):
                    normalized_per_page = []
                    for p in pdf_pages_all:
                        t = p.get("text", "") or ""
                        if t.strip():
                            normalized_per_page.append(normalize_with_ai(api_key, model, t))
                        else:
                            normalized_per_page.append("")  # trang scan ch∆∞a OCR => ƒë·ªÉ tr·ªëng
            else:
                normalized_per_page = None

            docx_bytes = build_docx_from_pdf_pages(
                title=title,
                pages=pdf_pages_all,
                normalized_texts=normalized_per_page,
                embed_images=embed_images,
                page_image_width_in=6.5,
            )

            # Preview nhanh: gh√©p text (ƒë·ªÉ th·∫ßy th·∫•y kh√¥ng ch·ªâ c√≥ ·∫£nh)
            preview_text = "\n\n".join([(normalized_per_page[i] if normalized_per_page else p["text"]) for i, p in enumerate(pdf_pages_all)]).strip()
            st.markdown("### Xem tr∆∞·ªõc (text tr√≠ch xu·∫•t t·ª´ PDF)")
            st.text_area("Preview", preview_text if preview_text else "(Kh√¥ng c√≥ text ‚Äî PDF d·∫°ng scan. B·∫≠t OCR n·∫øu mu·ªën th·ª≠.)", height=260)

            st.success("Xong PDF ‚Üí Word.")
            st.download_button(
                "‚¨áÔ∏è T·∫£i Word t·ª´ PDF",
                data=docx_bytes,
                file_name="pdf_to_word.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            )

        # ===== Images ‚Üí Word =====
        if img_only:
            with st.spinner("ƒêang t·∫°o Word t·ª´ ·∫£nh ..."):
                docx_bytes2, preview2 = build_docx_from_images(
                    title=title if not pdf_pages_all else (title + " (·∫¢nh)"),
                    images=img_only,
                    use_ocr=use_ocr,
                    use_ai=use_ai and bool(api_key.strip()),
                    api_key=api_key,
                    model=model,
                    embed_images=embed_images,
                    image_width_in=6.5,
                )

            st.markdown("### Xem tr∆∞·ªõc (text tr√≠ch xu·∫•t t·ª´ ·∫£nh)")
            st.text_area("Preview ·∫£nh", preview2 if preview2 else "(Kh√¥ng c√≥ text ‚Äî b·∫≠t OCR ƒë·ªÉ th·ª≠.)", height=220)

            st.success("Xong ·∫¢nh ‚Üí Word.")
            st.download_button(
                "‚¨áÔ∏è T·∫£i Word t·ª´ ·∫¢nh",
                data=docx_bytes2,
                file_name="images_to_word.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            )

st.divider()
st.caption(
    "B·∫£n n√†y c·ªë √Ω gi·ªëng b·∫£n ƒë·∫ßu: ∆∞u ti√™n text layer. "
    "Trang scan s·∫Ω kh√¥ng l√†m app ch·∫øt; n·∫øu kh√¥ng OCR th√¨ v·∫´n ƒë√≠nh k√®m ·∫£nh v√† ghi ch√∫."
)
